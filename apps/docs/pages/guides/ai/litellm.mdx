import Layout from '~/layouts/DefaultGuideLayout'
import { Tabs } from 'ui'
export const TabPanel = Tabs.Panel

export const meta = {
  id: 'ai-litellm',
  title: 'LiteLLM ðŸš…',
  description:
    'Learn how to integrate Supabase with LiteLLM, a simple model I/O library to standardize LLM API calls',
  sidebar_label: 'LangChain',
}

LiteLLM is a simple package to make LLM API Calls (Anthropic, Azure, OpenAI, etc.). LiteLLM supports using Supabase to automatically **log your calls and cost per call across LLM providers**. 

## Initializing your database

Prepare you database by creating a 'request_logs' table:

<Tabs
  scrollable
  size="small"
  type="underlined"
  defaultActiveId="sql"
>
<TabPanel id="sql" label="SQL">

```sql
create table
  public.request_logs (
    id bigint generated by default as identity,
    created_at timestamp with time zone null default now(),
    model text null default ''::text,
    messages json null default '{}'::json,
    response json null default '{}'::json,
    end_user text null default ''::text,
    error json null default '{}'::json,
    response_time real null default '0'::real,
    total_cost real null,
    constraint request_logs_pkey primary key (id)
  ) tablespace pg_default;
```

</TabPanel>
</Tabs>

## Usage

LiteLLM provides `success_callbacks` and `failure_callbacks`, making it easy for you to send data to a particular provider depending on the status of your responses.

In this case, we want to log requests to Supabase in both scenarios.

### Use Callbacks

Use just 2 lines of code, to instantly see costs and log your responses across all providers with Supabase:

```python
litellm.success_callback=["supabase"]
litellm.failure_callback=["supabase"]
```

Complete Code 
```python
from litellm import completion

## set env variables
os.environ["SUPABASE_URL"] = "your-supabase-url" 
os.environ["SUPABASE_KEY"] = "your-supabase-key" 
os.environ["OPENAI_API_KEY"] = "your-openai-key"

# set callbacks
litellm.success_callback=["supabase"]
litellm.failure_callback=["supabase"]

#openai call
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}]) 

#bad call
response = completion(model="chatgpt-test", messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm a bad call to test error logging"}]) 
```



## And that's it ðŸ˜Š

In just **10 lines of code**, we were able to start accepting requests from **5+ LLM providers** and get **instant logging** for our calls and cost per provider.

## Resources

- Official [LiteLLM Github](https://github.com/BerriAI/litellm).
- Official [LiteLLM docs](https://litellm.readthedocs.io/en/latest/).

export const Page = ({ children }) => <Layout meta={meta} children={children} />

export default Page